<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>Adapter Tuning</title>
  <link rel="stylesheet" type="text/css" href="build/build.css">
  <link rel="stylesheet" type="text/css" href="styles/prism.css">
  <link rel="stylesheet" type="text/css" href="styles/custom.css">

</head>
<body>  
<article class="deck">

  <!-- Title Slide -->
    <section class="slide">
      <div class="image-container">
        <img src="images/intro_image.jpg" alt="Intro Image">
      </div>
      <div class="text-container">
        <h2>Adapter Tuning</h2>
        <div> 
          <p>Become <span class="dynamic-text"></span></p>
        </div>
      </div>
    </section>  
    
    <!-- Challenges -->
    <section class="slide">
      <h2>üî• Challenges of Fine-tuning Transformers üî•</h2>
      <ul class="challenge-list">
          <li><b>High Compute Costs:</b> Training is expensive and slow.</li>
          <li><b>Huge Storage Needs:</b> Billions of parameters require massive storage.</li>
          <li><b>Catastrophic Forgetting:</b> Models can forget prior knowledge.</li>
          <li><b>Data Hungry:</b> Requires extensive task-specific data.</li>
          <li><b>Hyperparameter Sensitivity:</b> Difficult to tune optimally.</li>
      </ul>
    </section>
    
    <!-- Overview -->    
    <section class="slide">
      <h2>Adapter Overview</h2>
      <p><b>Gradient decent update</b></p>
      <p class="equation"> 
          Œ∏<sub>t</sub> ‚Üê Œ∏<sub>t-1</sub> - Œ±‚àá<sub>Œ∏</sub>‚Ñí|<sub>Œ∏=Œ∏<sub>t-1</sub></sub>
          </p>
          <p><b>trained model</b></p>
          <p class="equation">
          Œ∏<sub>T</sub> = Œ∏<sub>0</sub> - Œ±‚àá<sub>Œ∏</sub>‚Ñí|<sub>Œ∏=Œ∏<sub>0</sub></sub> - Œ±‚àá<sub>Œ∏</sub>‚Ñí|<sub>Œ∏=Œ∏<sub>1</sub></sub> ... - Œ±‚àá<sub>Œ∏</sub>‚Ñí|<sub>Œ∏=Œ∏<sub>T-1</sub></sub>
          </p>
      <p><b>A fine-tuned model</b></p>
      <p class="equation">
        Œ∏<sub>F</sub> = 
        <span class="boxed trained-model">
          Œ∏<sub>0</sub> ‚àí Œ± Œ£<sup>T-1</sup> ‚àá<sub>Œ∏</sub>‚Ñí|<sub>Œ∏=Œ∏<sub>t</sub></sub> 
          <br><small>trained model</small>
        </span>
        <span class="boxed fine-tuning">
          ‚àí Œ± Œ£<sup>T+F</sup> ‚àá<sub>Œ∏</sub>‚Ñí|<sub>Œ∏=Œ∏<sub>t</sub></sub>
          <br><small>fine-tuning data</small>
        </span>
      </p>
    </section>
    
      <!-- LoRA-->
    <section class="slide">
        <h1> LoRA </h1>
      <div class="image-container">  
        <img src="images/lora.jpg" alt="LoRA Diagram">
      </div>
      <ul class="lower-bullets">  
        <li><b>ŒîW has R x C =  10K x 20K = 200M elements</b></li>
        <li><b>B has R x r = 10K x 4 = 40K elements</b></li>
        <li><b>A has r x C= 20K x 4 = 80K elements</b></li>
      </ul>
    </section>   
    
     <!-- Types -->
<section class="slide">
      <h2>Overview of Adapter Types</h2>
      <div class="adapter-card">
        <h3>Introduction</h3>
        <p>Adapters are a recent innovation in transfer learning for NLP that offer a lightweight alternative to fine-tuning. This section provides an overview of common adapter architectures.</p>
      </div>
    </section>

    <section class="slide">
      <div class="adapter-card">
        <h3>Houlsby Adapter</h3>
        <p>Inserts two adapter layers after the feed-forward network within each transformer block.</p>
        <ul>
          <li><b>Placement:</b> After both the attention layer and the FFN layer.</li>
          <li><b>Design:</b> Bottleneck architecture with two layers, one down-projecting and one up-projecting.</li>
          <li><b>Pros:</b> Relatively high performance.</li>
          <li><b>Cons:</b> Adds more parameters compared to other adapter types.</li>
        </ul>
      </div>
    </section>

    <section class="slide">
      <div class="adapter-card">
        <h3>Bottleneck Adapter (Pfeiffer Adapter)</h3>
        <p>The most common type, adds a bottleneck adapter after the feed-forward layer of each transformer block.</p>
        <ul>
          <li><b>Placement:</b> After FFN or the attention layer.</li>
          <li><b>Design:</b> Down-projects the input to a smaller dimension, applies a non-linearity, and then up-projects back to the original dimension.</li>
          <li><b>Pros:</b> Good performance-parameter efficiency trade-off.</li>
          <li><b>Cons:</b> Might be less effective for tasks heavily relying on attention mechanisms.</li>
        </ul>
      </div>
    </section>

    <section class="slide">
      <div class="adapter-card">
        <h3>Parallel Adapter</h3>
        <p>Adds an adapter in parallel to the feed-forward network or attention layer.</p>
        <ul>
          <li><b>Placement:</b> Parallel to the FFN or the attention layer.</li>
          <li><b>Design:</b> Similar to bottleneck adapter but applied in parallel. The outputs are combined, often through addition.</li>
          <li><b>Pros:</b> Can achieve better performance than sequential adapters.</li>
          <li><b>Cons:</b> More complex to implement.</li>
        </ul>
      </div>
    </section>

    <section class="slide">
      <div class="adapter-card">
        <h3>LoRA (Low-Rank Adaptation)</h3>
        <p>Decomposes the weight updates into two smaller, low-rank matrices. Often used to reduce the number of trainable parameters in fine-tuning.</p>
        <ul>
          <li><b>Placement:</b> In parallel to the weight matrices of different layers (e.g. query, key, value matrices of the attention layer).</li>
          <li><b>Design:</b> Introduces two matrices A and B. A down-projects, B up-projects, and their product approximates the weight update.</li>
          <li><b>Pros:</b> Extremely parameter-efficient.</li>
          <li><b>Cons:</b> Might require careful tuning of the rank hyperparameter.</li>
        </ul>
      </div>
    </section>
    
    <!-- Parameter efficiency -->
    <section class="slide parameter-efficiency-slide">
         <h2>Parameter Efficiency</h2>
         <div class="content">
           <div class="image-container">
             <img src="images/parameter_efficiency.jpg" alt="Parameter Efficiency">
           </div>
           <div class="text-content">
             <table class="comparison-table">
               <thead>
                 <tr>
                   <th>Model</th>
                   <th>Trainable Parameters</th>
                 </tr> 
               </thead>
               <tbody>
                 <tr>
                   <td>Fine-tuning</td>
                   <td>~110M (full model)</td>
                 </tr>
                 <tr>
                   <td>Adapter</td>
                   <td>~1-3M (adapter layers only)</td>
                 </tr>
                 <tr>
                   <td>LoRA</td>
                   <td>~0.1M (low-rank matrices)</td>
                 </tr>
               </tbody>
             </table>
           </div>
         </div>
     </section>
     
    <!-- Data Prep -->
    <section class="slide"> 
      <h2>Data Preparation</h2>
          <pre><code class="language-python" style="font-size: 0.6em; white-space: pre-wrap; word-wrap: break-word; font-family: 'Source Code Pro', monospace; max-width: 100%; width: 95vw;">
    
     def create_messages_and_prompt(comment, label):
         """Creates the messages list and formatted prompt for a single comment."""
         formatted_content = content.format(comment.replace("\n", " "))

         messages = [
             {"role": "user",
              "content": formatted_content 
             },
             {"role": "assistant",
              "content": str(label)
             }
         ]
         return messages

     # Function to create the prepared fine-tuning data from dataframe
     def prepare_fine_tuning_data(df):
         """Prepares the fine-tuning data from a Pandas DataFrame."""
         fine_tuning_data = []

         for _, row in df.iterrows():
             comment = row['comments']
             label = int(row['labels'])
             messages = create_messages_and_prompt(comment, label)
             fine_tuning_data.append(messages)
         return fine_tuning_data</code></pre>
    
    </section> 
         
    <!-- Architecture -->
    <section class="slide"> 
      <h2>Basic Architecture</h2>
          <pre><code class="language-python" style="font-size: 0.6em; white-space: pre-wrap; word-wrap: break-word; font-family: 'Source Code Pro', monospace; max-width: 100%; width: 95vw;">
    
     def timer(func):
         """Prints the runtime of the decorated function."""
         @wraps(func)
         def wrapper_timer(*args, **kwargs):
             start_time = time.perf_counter()  # Start time
             value = func(*args, **kwargs)
             end_time = time.perf_counter()    # End time
             run_time = end_time - start_time  # Run time
             print(f"Finished {func.__name__!r} in {run_time:.4f} secs")
             return value
         return wrapper_timer

     def check_and_log_gpu_memory(threshold_mb: Optional[int] = None):
         """
         Decorator to check and log GPU memory usage before and after function execution.

         Args:
             threshold_mb (Optional[int]): If provided, raises an error if the available 
                                            GPU memory is below this threshold (in MB) 
                                            before the function executes.
         """
         def decorator(func):
             @wraps(func)
             def wrapper(*args, **kwargs):
                 if torch.cuda.is_available():
                     device = torch.cuda.current_device()

                     def get_memory_usage():
                         memory_stats = torch.cuda.memory_stats(device=device)
                         current_allocated = memory_stats["allocated_bytes.all.current"] / (1024**2)
                         return current_allocated

                     # Check memory before function execution
                     memory_before = get_memory_usage()
                     print(f"GPU Memory Used (Before {func.__name__}): {memory_before:.2f} MB")

                     if threshold_mb is not None and memory_before < threshold_mb:
                         raise RuntimeError(
                             f"Available GPU memory ({memory_before:.2f} MB) is below "
                             f"the threshold ({threshold_mb} MB) before executing {func.__name__}."
                         )

                     # Execute the function
                     result = func(*args, **kwargs)

                     # Check memory after function execution
                     memory_after = get_memory_usage()
                     print(f"GPU Memory Used (After {func.__name__}): {memory_after:.2f} MB")
                 else:
                     print("CUDA is not available. GPU memory check skipped.")
                     result = func(*args, **kwargs)

                 return result
             return wrapper
         return decorator



     class PhiModel:
         """
         A class for performing text analysis,
         specifically for evaluating justifications
         provided in exception reports using a pre-trained language model (Phi-3).
         """

         def __init__(
                self,
                model_name: str = "microsoft/Phi-3.5-mini-instruct",
                tokenizer_path: str = "models/phi_3_instruct_tokenizer",
                peft_model_id: Optional[str] = None,
                data: Optional[pd.DataFrame] = None,
                message: Optional[List[Dict[str, str]]] = None,
                ):
             """
             Initializes the phiModel class.

             Args:
                 model_name (str): The name of the base pre-trained
                 language model to use.
                 tokenizer_path (str): The path to the tokenizer to use.
                 peft_model_id (str, optional): The path or id of the PEFT model.
                 If None, uses the base model. Defaults to None.
                 data (pd.DataFrame, optional): The input DataFrame
                 containing the text data. Defaults to None.
             """

             self.data = Dataset.from_pandas(data)
             bnb_config = BitsAndBytesConfig(
                 load_in_8bit=True,
                 #llm_int8_enable_fp32_cpu_offload=True,  # Enable CPU offloading!!
                 #bnb_4bit_quant_type="nf4",  # or "fp4"
                 #bnb_4bit_use_double_quant=True,
                 #bnb_4bit_compute_dtype=torch.bfloat16,
             )

             # Load base model and tokenizer
             self.model = AutoModelForCausalLM.from_pretrained(
                     model_name,
                     device_map="cuda" if torch.cuda.is_available()
                     else "cpu",  # Use gpu if available
                     torch_dtype="auto",
                     trust_remote_code=True,
                     attn_implementation="eager",
                     quantization_config=bnb_config
                     )

             phi_tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

             if peft_model_id:

                 self.phi_pipe = pipeline(
                     "text-generation",
                     model=self.model,
                     tokenizer=phi_tokenizer,
                     torch_dtype=torch.bfloat16
                 )
                 self.phi_pipe.model = PeftModel.from_pretrained(
                     self.model, peft_model_id)

             else:
                 # Create a text generation pipeline
                 self.phi_pipe = pipeline(
                     "text-generation",
                     model=self.model,
                     tokenizer=phi_tokenizer,
                     torch_dtype=torch.bfloat16
                 )

             # Define generation arguments
             self.generation_args = {
                 "max_new_tokens": 2,  # Limit generated text length
                 "return_full_text": False,  # Only return generated part
                 "temperature": 0.001,  # (lower = more deterministic)
                 "do_sample": True,  # more diverse outputs
             }

             self.message = message
             
         @timer
         @check_and_log_gpu_memory(threshold_mb=500) 
         def find_justification(self,text: str, max_length: int = 4096) -> int:
             """
             Evaluates the justification provided in the input 'text' using the
             Phi-3 model and returns a grade (0-3) indicating its adequacy.

             Args:
                 text (str): The justification text to be evaluated.

             Returns:
                 int: The generated grade (0-1) as a integer.
                  Returns -1 if the generation fails or
                  the model produces an invalid output.
             """
             tokens = self.phi_pipe.tokenizer.tokenize(text)
             if len(tokens) > max_length:
                 tokens = tokens[:max_length]  # Truncate the tokens
                 text = self.phi_pipe.tokenizer.convert_tokens_to_string(tokens)

             messages = copy.deepcopy(self.message)
             messages[0]['content'] = messages[0]['content'] + text

             try:
                 output = self.phi_pipe(messages,
                                        **self.generation_args)[0]['generated_text']
                 grade = int(output.strip())   # Convert to integer
                 return grade if 0 <= grade <= 3 else -1
             except (ValueError, IndexError) as e:
                 print(f"Error during text generation: {e}")
                 return -1

         def get_parameter_count(self) -> Optional[int]:
             """
             Calculates and returns the total number of
             parameters in the loaded Phi-3 model.

             Returns:
                 int: The total number of parameters in the model.
                 Returns None if the model is not loaded correctly
             """
             if not hasattr(self, 'model') or self.model is None:
                 print("Model not loaded correctly. Cannot return parameter count.")
                 return None

             total_params = sum(p.numel() for p in self.model.parameters())
             return total_params


     @timer
     @check_and_log_gpu_memory() 
     def process_data_in_chunks(pModel: PhiModel, chunk_size: int = 1):
         """
         Processes data in chunks, prints memory usage,
         and applies find_justification.
         """
         outputArray = []
         total_len = len(pModel.data)

         for i in range(0, total_len, chunk_size):
             gc.collect()
             torch.cuda.empty_cache()

             chunk_end = min(i + chunk_size, total_len)
             chunk = pModel.data['REMARKS_COMMENTS'][i:chunk_end]

             temp_output_array = []
             for j, text in enumerate(chunk):
                 try:
                     outputString = pModel.find_justification(text)
                     temp_output_array.append(outputString)
                     print(i, j, temp_output_array)
                     # print(torch.cuda.memory_summary())

                 except Exception as e:
                     print(f"Error processing row {i + j}: {e}")
                     temp_output_array.append("None")

             outputArray.extend(temp_output_array)

             del chunk
             del temp_output_array

         new_data = pModel.data.add_column('ca_remarks_justification', outputArray)
         pd.DataFrame(new_data).to_json('data/phi3_justifications_remarks.json')

         del new_data
         gc.collect()</code></pre>
    
    </section> 
    <!-- adapter placement -->
  <section class="slide">  
    <h2>Adapter Placement</h2>
        <pre><code class="language-python" style="font-size: 0.6em; white-space: pre-wrap; word-wrap: break-word; font-family: 'Source Code Pro', monospace; max-width: 100%; width: 95vw;">
   class MyLoggingCallback(TrainerCallback):
       """
       A custom callback for logging specific training metrics
       at each logging step.

       This callback extends the `TrainerCallback` class and overrides
       the `on_log` and `on_step_end` methods to customize the logging
       behavior during training.
       It logs information such as training loss, evaluation loss,
       evaluation runtime, and evaluation samples/steps per second.
       """

       def on_log(
           self,
           args: TrainingArguments,
           state: TrainerState,
           control: TrainerControl,
           logs: Optional[Dict[str, float]] = None,
           **kwargs: Any,
       ):
           """
           Event called after logging the last logs.

           Args:
               args (TrainingArguments): The training arguments used
                   for the current training run.
               state (TrainerState): The current state of the trainer.
               control (TrainerControl): The control object that enables
                   modification of training behavior.
               logs (dict, optional): The logs to be recorded. Defaults to None.
           """

           _ = logs.pop("total_flos", None)
           if state.is_local_process_zero:
               print(logs)

       def on_step_end(
           self,
           args: TrainingArguments,
           state: TrainerState,
           control: TrainerControl,
           model: Optional[PreTrainedModel] = None,
           **kwargs: Any,
       ):
           """
          Event called at the end of each training step.

          Args:
              args (TrainingArguments): The training arguments used for the
                  current training run.
              state (TrainerState): The current state of the trainer.
              control (TrainerControl): The control object that enables
                  modification of training behavior.
              model (PreTrainedModel, optional): The model being trained.
                  Defaults to None.
          """

           if state.global_step % args.logging_steps == 0:
               logs = {}
               logs["step"] = state.global_step

               if len(state.log_history) > 0:
                   if 'loss' in state.log_history[-1]:
                       logs["loss"] = state.log_history[-1]['loss']
                   if 'grad_norm' in state.log_history[-1]:
                       logs["grad_norm"] = state.log_history[-1]['grad_norm']
                   if 'eval_loss' in state.log_history[-1]:
                       logs["eval_loss"] = state.log_history[-1]['eval_loss']
                   if 'eval_runtime' in state.log_history[-1]:
                       logs["eval_runtime"] = \
                           state.log_history[-1]['eval_runtime']
                   if 'eval_samples_per_second' in state.log_history[-1]:
                       logs["eval_samples_per_second"] = \
                           state.log_history[-1]['eval_samples_per_second']
                   if 'eval_steps_per_second' in state.log_history[-1]:
                       logs["eval_steps_per_second"] = \
                           state.log_history[-1]['eval_steps_per_second']

               # Log the metrics using the Trainer's log method
               control.should_log = True

           return control


   class PhiTrainer:
       """
       A class for fine-tuning the Phi model using LoRA (Low-Rank Adaptation).

       This class handles the entire process of preparing the data,
       configuring the model, setting up the trainer, and executing the
       training loop for fine-tuning the Phi model with LoRA applied to
       specific modules.
       """

       def __init__(
           self,
           base_model: Any,  # Replace Any with the actual type of your base model if known
           tokenizer_path: str = "models/phi_3_instruct_tokenizer",
           finetune_data_path: str = "data/adapterDataTuning.csv",
           base_data_path: str = "data/ca_data_v3.csv",
           output_dir: str = "output",
           run_name: str = "run1",
           target_modules: List[str] = [
               "q_proj",
               "k_proj",
               "v_proj",
               "o_proj",
               "gate_proj",
               "down_proj",
               "up_proj",
               "lm_head",
           ],
       ):

           self.base_model = base_model
           self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
           torch.random.manual_seed(0)
           self.tokenizer_path = tokenizer_path
           self.finetune_data_path = finetune_data_path
           self.base_data_path = base_data_path
           self.target_modules = target_modules
           self.output_dir = output_dir
           self.run_name = run_name
           self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
           self.accelerator = Accelerator()
           self.logging_steps = 1

           # Hyperparameters
           self.per_device_train_batch_size = 4
           self.gradient_accumulation_steps = 2
           self.optim = 'adamw_torch_fused'
           self.learning_rate = 1e-4
           self.max_grad_norm = 0.3
           self.warmup_ratio = 0.03
           self.lr_scheduler_type = "linear"
           self.num_train_epochs = 1.0

           # lora hyperparameters
           self.lora_dropout = 0.05
           self.r = 16

           self.tokenizer_path = "models/phi_3_instruct_tokenizer"

           self.tokenizer = AutoTokenizer.from_pretrained(
               self.tokenizer_path,
               model_max_length=512,
               padding_side="right",
               add_eos_token=True)

           self.tokenizer.pad_token = self.tokenizer.eos_token
           self.accelerator = Accelerator()
           self.prepare_data()
           self.setup_model()
           self.setup_trainer()

       def update_hyperparameters(self,  params: Dict[str, Any]):
           """
           Updates the hyperparameters of the trainer.

           Args:
               params (dict): A dictionary containing the
               hyperparameters to update.
           """

           for key, value in params.items():
               setattr(self, key, value)  # Directly sets the attribute

           self.setup_model()  # recreate the model and apply new LoRA configs
           self.setup_trainer()

       def tokenize(self, prompt: Dict[str, str]) -> Dict[str, List[int]]:
           """
           Tokenizes a single prompt.

           Args:
               prompt (dict): A dictionary containing the text to tokenize.

           Returns:
               dict: The tokenized prompt with input_ids and attention_mask.
           """

           result = self.tokenizer(
               prompt['text'],
               truncation=True,
               max_length=512,
               padding="max_length",
           )
           result["labels"] = result["input_ids"].copy()
           return result

       def prepare_data(self):
           """
           Prepares the training and evaluation datasets
           by loading and tokenizing the data.
           """

           df_finetune = pd.read_csv(self.finetune_data_path)
           df_finetune = df_finetune[['Prompt', 'Response']]
           df_finetune['text'] = df_finetune["Prompt"]+df_finetune["Response"]
           df_finetune.drop(columns=['Prompt', 'Response'], inplace=True)
           dataset = Dataset.from_pandas(df_finetune)

           # Apply the tokenization function to the entire dataset
           self.tokenized_datasets = dataset.map(
               self.tokenize,
               batched=True,
               remove_columns=["text"],  # Remove the original text column
           )
           self.tokenized_datasets = self.tokenized_datasets.train_test_split(
               test_size=0.1, seed=42)

       def setup_model(self):
           """
           Sets up the model by applying LoRA configurations
           and enabling gradient checkpointing.
           """

           base_model = self.base_model.model

           for name, param in base_model.named_parameters():
               param.requires_grad = False  # Freeze all base model layers

           lora_config = LoraConfig(
               r=self.r,
               target_modules=self.target_modules,
               lora_alpha=8,
               lora_dropout=self.lora_dropout,
               bias="none",
               task_type="CAUSAL_LM",
               init_lora_weights=True,
               )

           if hasattr(base_model, "enable_input_require_grads"):
               base_model.enable_input_require_grads()
           else:
               def make_inputs_require_grad(module, input, output):
                   output.requires_grad_(True)

               base_model.get_input_embeddings().register_forward_hook(
                   make_inputs_require_grad)

           self.model = get_peft_model(base_model, lora_config)
           for name, param in self.model.named_parameters():
               if 'lora' in name:  # Check if LoRA parameter
                   param.requires_grad = True

           self.model.gradient_checkpointing_enable()

           self.model.print_trainable_parameters()
   #          Print parameter names and requires_grad status
   #         for name, param in self.model.named_parameters():
   #            print(f"  {name}: requires_grad={param.requires_grad}")

       def setup_trainer(self):
           """
           Sets up the SFTTrainer with the appropriate
           training arguments and datasets.
           """
           training_args = TrainingArguments(
               output_dir=self.output_dir,
               save_strategy="steps",
               evaluation_strategy=IntervalStrategy.STEPS,
               save_steps=self.logging_steps,
               eval_steps=self.logging_steps,
               logging_steps=self.logging_steps,
               do_eval=True,
               num_train_epochs=self.num_train_epochs,
               per_device_train_batch_size=self.per_device_train_batch_size,
               gradient_accumulation_steps=self.gradient_accumulation_steps,
               gradient_checkpointing=True,
               optim=self.optim,
               learning_rate=self.learning_rate,
               fp16=True,  # Check if your setup supports fp16
               max_grad_norm=self.max_grad_norm,
               warmup_ratio=self.warmup_ratio,
               group_by_length=True,
               lr_scheduler_type=self.lr_scheduler_type)

           lora_params = []
           for name, param in self.model.named_parameters():
               if param.requires_grad:
                   lora_params.append(param)

   #        self.optimizer = AdamW(lora_params, lr=self.learning_rate)

           self.trainer = SFTTrainer(  # Initialize your SFTTrainer
               self.model,
               train_dataset=self.tokenized_datasets['train'],
               eval_dataset=self.tokenized_datasets['test'],
               args=training_args,
               callbacks=[MyLoggingCallback()],
               #           optimizers=(self.optimizer, None,)
               #           data_collator=data_collator
           )

   #         #self.trainer.processing_class = data_collator

       def train(self):
           """
           Starts the training process and prints the
           gradients of the model parameters.
           """
           self.trainer.train()
           # Print gradients after a few steps

           for name, param in self.model.named_parameters():
               if param.grad is not None:
                   print(f"Gradient of {name}:\n{param.grad}\n")</code></pre>
      </section>
  </section>
    <!-- Training -->
    <section class="slide"> 
      <h2>Training Process</h2>
          <pre><code class="language-python" style="font-size: 0.6em; white-space: pre-wrap; word-wrap: break-word; font-family: 'Source Code Pro', monospace; max-width: 100%; width: 95vw;">

     class HyperparameterSearch:
         """
         A class for performing hyperparameter search using random search
         and MLflow for tracking.

         This class facilitates the exploration of a defined hyperparameter space
         by training and evaluating
         a model with different hyperparameter combinations. It uses MLflow
         to log parameters, metrics, and models for each trial, helping to
         identify the best hyperparameter set based on evaluation loss.

         Attributes:
             trainer (object): An instance of a trainer class (e.g., PhiTrainer)
             that has methods for updating hyperparameters, training,
             and evaluating the model.
             search_space (dict): A dictionary defining the hyperparameter
             search space. Keys are hyperparameter names, and values are
             either lists of discrete values or tuples representing the lower
             and upper bounds of a continuous range.
             num_trials (int): The number of hyperparameter combinations
             to evaluate.
             best_loss (float): The best evaluation loss found so far.
             Initialized to positive infinity.
             best_params (dict): The hyperparameter combination that
             resulted in the best loss.
         """

         def __init__(self,
                      trainer,
                      search_space,
                      num_trials=3):
             self.trainer = trainer
             self.search_space = search_space
             self.num_trials = num_trials
             self.best_loss = float('inf')  # Initialize with a very high loss
             self.best_params = {}

         def train_and_evaluate(self, params):
             """
             Trains and evaluates the model with a given set of hyperparameters.

             This method updates the trainer with the new hyperparameters,
             trains the model,
             evaluates it, logs parameters and metrics to MLflow, and saves
             the best model based on evaluation loss.

             Args:
                 params (dict): A dictionary containing the hyperparameter
                 combination to use for this trial.

             Returns:
                 float: The evaluation loss for this trial, or infinity
                 if evaluation loss is not available.
             """

             try:
                 mlflow.set_experiment("phi_hyperparam_search")
             except Exception:
                 experiment_id = mlflow.create_experiment("phi_hyperparam_search")
                 mlflow.set_experiment(experiment_id=experiment_id)

             with mlflow.start_run():
                 print(f"Training with parameters: {params}")

                 p = self.trainer  # Create a new PhiTrainer instance
                 p.update_hyperparameters(params)  # Update hyperparameters

                 # p.setup_model()

                 for key, value in params.items():  # Log parameters
                     mlflow.log_param(key, value)

                 p.train()  # Train the model with the given parameters

                 # Evaluate the model
                 eval_metrics = p.trainer.evaluate()
                 # Get eval loss or set to inf if not available
                 eval_loss = eval_metrics.get("eval_loss", float('inf'))
                 print('loss--' + str(eval_loss))
                 mlflow.log_metric("eval_loss", eval_loss)  # Log the eval loss

                 if eval_loss < self.best_loss:
                     self.best_loss = eval_loss
                     self.best_params = params
                     p.model.save_pretrained("models/lora_best")  # Save best model

                 return eval_loss

         def run_search(self):
             """
            Runs the hyperparameter search for the specified number of trials.

            This method iterates through the specified number of trials,
            randomly sampling hyperparameter combinations from the defined
            search space. For each combination, it calls `train_and_evaluate`
            to train and evaluate the model, ultimately identifying and
            printing the best hyperparameter set and its corresponding
            evaluation loss.
            """
             for _ in range(self.num_trials):  # loop through all the search space
                 params = {}
                 for param_name, param_range in self.search_space.items():
                     if isinstance(param_range, list):  # parameter is discrete.
                         params[param_name] = random.choice(param_range)
                     else:  # parameter is continous
                         # take a uniform sample between min and max.
                         params[param_name] = random.uniform(*param_range)

                 self.train_and_evaluate(params)  # calls MLflow inside

             print(f"Best loss: {self.best_loss}")
             print(f"Best parameters: {self.best_params}")</code></pre>

    </section> 
    <!-- Other areas --> 
<section class="slide">
  <h2>Adapter Tuning, Pruning, and Distillation</h2>

  <div style="display: flex; justify-content: space-around;">

    <div style="flex: 1; margin-right: 10px;">
      <h3>Adapter Tuning</h3>
      <ul>
        <li><strong>What:</strong> Add small, trainable modules to a frozen model.</li>
        <li><strong>How:</strong> New parameters; base model unchanged.</li>
      </ul>
    </div>

    <div style="flex: 1; margin-right: 10px;">
      <h3>Pruning</h3>
      <ul>
        <li><strong>What:</strong> Remove less important parts of a model.</li>
        <li><strong>How:</strong> Remove existing parameters.</li>
      </ul>
    </div>

    <div style="flex: 1;">
      <h3>Distillation</h3>
      <ul>
        <li><strong>What:</strong> Train a small "student" model from a large "teacher".</li>
        <li><strong>How:</strong> Student learns from teacher's outputs.</li>
      </ul>
    </div>

  </div>

  <h3>How They Work Together:</h3>

  <ul>
    <li><strong>Prune, then adapt:</strong> Prune a base model, then add adapters for efficiency.</li>
    <li><strong>Distill, then adapt:</strong> Distill a smaller base, then add adapters.</li>
    <li><strong>Prune adapters:</strong>  Make adapters smaller after training.</li>
    <li><strong>Prune, then distill:</strong> Prune a teacher, then distill to a student.</li>
  </ul>

</section>
</article>

<script src="build/build.js"></script>
<script src="scripts/dynamic.js"></script>
<script src="scripts/prism.js"></script>

</body>
</html>